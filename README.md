ğŸ¤– AI Assistant com LangChain + Ollama + FastAPI

Este projeto implementa um assistente de IA que pode usar ferramentas externas (como calculadora, busca, etc.) atravÃ©s do LangChain.
Ele utiliza o Ollama como LLM local, expÃµe um endpoint em FastAPI (/ask) e segue boas prÃ¡ticas de modularidade e seguranÃ§a (configuraÃ§Ã£o via .env).

âš¡ï¸ Requisitos

Python 3.12+

Poetry
 ou venv para gerenciar dependÃªncias

Ollama
 instalado e rodando localmente

Modelos do Ollama (exemplo: mistral, llama3 ou outro de sua preferÃªncia)

ğŸ“¦ InstalaÃ§Ã£o

Clone o repositÃ³rio:
git clone https://github.com/SilasGuilherme/ai-assistant-multiple-tools.git
cd ai-assistant

Crie e ative um ambiente virtual:
python -m venv .venv
source .venv/bin/activate   # Linux/macOS
.venv\Scripts\activate      # Windows

Instale as dependÃªncias:
pip install -r requirements.txt

âš™ï¸ ConfiguraÃ§Ã£o

O projeto utiliza variÃ¡veis de ambiente apenas para as integraÃ§Ãµes externas (APIs opcionais).

A porta **nÃ£o Ã© configurada no cÃ³digo nem no `.env`**.  
Por padrÃ£o, ao rodar com Uvicorn, a aplicaÃ§Ã£o sobe em `http://127.0.0.1:8000`.  

Se quiser usar outra porta, especifique no comando de execuÃ§Ã£o:
uvicorn app.main:app --reload --port 9000

O modelo do Ollama estÃ¡ definido no cÃ³digo, em:
app/core/config.py

Por padrÃ£o usamos o modelo `mistral`.  
Se quiser trocar (ex.: para `llama2`), basta alterar esse arquivo.

## APIs Externas

O projeto possui integraÃ§Ã£o opcional com APIs de **Clima** e **NotÃ­cias**.  
Essas funcionalidades sÃ³ funcionam se vocÃª configurar as chaves de API corretamente.

### 1. Clima
Usamos a API [OpenWeatherMap](https://openweathermap.org/api).  
Crie uma conta gratuita e obtenha sua chave.  

No arquivo .env, adicione:
OPENWEATHER_API_KEY=sua_chave_aqui

### 2. NotÃ­cias
Usamos a API [NewsAPI](https://newsapi.org/).  
Crie uma conta gratuita e copie a chave.

No arquivo .env, adicione:
NEWS_API_KEY=sua_chave_aqui

### Uso sem chaves
Se vocÃª nÃ£o configurar as chaves, o projeto **continua funcionando** com o modelo Mistral no Ollama, mas os comandos de clima e notÃ­cias retornarÃ£o uma mensagem de erro informando que a API nÃ£o estÃ¡ configurada.


â–¶ï¸ Como executar

Certifique-se que o Ollama estÃ¡ rodando:
ollama run mistral

Inicie a API:
uvicorn app.main:app --reload

Envie uma requisiÃ§Ã£o para o endpoint /ask:
curl -X POST http://127.0.0.1:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "Qual Ã© a raiz quadrada de 144?"}'

VocÃª receberÃ¡ a resposta processada pelo agente. ğŸ‰

ğŸ§© Estrutura do Projeto

app/
â”œâ”€â”€ agents/
â”‚ â”œâ”€â”€ router.py # ConfiguraÃ§Ã£o do agente e prompt
â”‚ â”œâ”€â”€ tools.py # Registro e definiÃ§Ã£o das ferramentas
â”‚
â”œâ”€â”€ core/
â”‚ â””â”€â”€ config.py # ConfiguraÃ§Ã£o principal do LLM (modelo Ollama)
â”‚ â””â”€â”€ logger.py # ConfiguraÃ§Ã£o de logs
â”‚
â”œâ”€â”€ services/
â”‚ â”œâ”€â”€ math_service.py # ServiÃ§o para cÃ¡lculos matemÃ¡ticos
â”‚ â”œâ”€â”€ news_service.py # ServiÃ§o para integraÃ§Ã£o com NewsAPI
â”‚ â””â”€â”€ weather_service.py # ServiÃ§o para integraÃ§Ã£o com OpenWeatherMap
â”‚
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_math.py # Testes do serviÃ§o de cÃ¡lculos
â”‚ â”œâ”€â”€ test_news.py # Testes do serviÃ§o de notÃ­cias
â”‚ â””â”€â”€ test_weather.py # Testes do serviÃ§o de clima
â”‚
â””â”€â”€ main.py # Ponto de entrada da aplicaÃ§Ã£o FastAPI

.env     # VariÃ¡veis de ambiente (nÃ£o versionado)
.env.example     # Exemplo de configuraÃ§Ã£o de variÃ¡veis de ambiente
requirements.txt     # DependÃªncias do projeto
README.md     # Este arquivo


ğŸš€ Exemplo de Uso

POST /ask
{
  "question": "Me diga quanto Ã© (25 * 4) / 2"
}

Resposta:
{
  "answer": "O resultado Ã© 50."
}

## ğŸ§ª Testes

Este projeto utiliza **pytest** para rodar os testes unitÃ¡rios.

### Executar todos os testes
pytest -v

### Executar testes de um arquivo especÃ­fico
pytest app/tests/test_math.py -v
pytest app/tests/test_news.py -v
pytest app/tests/test_weather.py -v

### Estrutura dos testes
test.math.py â†’ valida cÃ¡lculos matemÃ¡ticos.
test.news.py â†’ valida integraÃ§Ã£o com a API de notÃ­cias (inclui mock).
test.weather.py â†’ valida integraÃ§Ã£o com a API de clima (inclui mock).

ğŸ§  LÃ³gica de ImplementaÃ§Ã£o

LLM Local (Ollama) â†’ O modelo mistral Ã© usado como cÃ©rebro principal.
LangChain Agents â†’ O agente recebe o prompt estruturado, decide se precisa usar uma ferramenta e retorna a resposta final.
Ferramentas â†’ Calculadora local, API pÃºblica de PrevisÃ£o do Tempo e API pÃºblica de NotÃ­cias. Outras podem ser adicionadas facilmente em tools.py.
FastAPI â†’ ExpÃµe um endpoint simples (/ask) que recebe perguntas em JSON e retorna respostas do agente.

ğŸ“š O que eu aprendi

- Como integrar Ollama com LangChain via ChatOllama.
- DiferenÃ§a entre create_openai_tools_agent (que tambÃ©m funciona com Ollama) e os agentes mais antigos (initialize_agent).
- ImportÃ¢ncia de configurar variÃ¡veis de ambiente para manter o projeto seguro e configurÃ¡vel.
- Como lidar com erros comuns do LangChain (ex: placeholders ausentes no prompt).

ğŸ”® O que faria diferente com mais tempo

- Adicionar uma UI em React para conversar com o assistente de forma mais amigÃ¡vel.
- Implementar memÃ³ria de longo prazo com banco de dados vetorial.
- Criar mais ferramentas Ãºteis (ex: busca na web, integraÃ§Ã£o com mais APIs externas).
- Escrever testes mais complexos para garantir a robustez do agente.
- Criar logs para aprimorar o monitoramento e debugging.


## ğŸ“„ LicenÃ§a
Este projeto estÃ¡ licenciado sob a licenÃ§a MIT â€“ veja o arquivo [LICENSE](./LICENSE) para mais detalhes.

## ğŸ‘¥ CrÃ©ditos
Desenvolvido por SILAS GUILHERME com uso de **LangChain**, **FastAPI** e **Ollama**.
